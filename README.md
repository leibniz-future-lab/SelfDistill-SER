# SelfDistill-SER

This is a Python and PyTorch code for the self-distillation framework in our paper: 

<!--[Fast Yet Effective Speech Emotion Recognition with Self-distillation](https://arxiv.org/pdf/2210.14636.pdf).-->


>Zhao Ren, Thanh Tam Nguyen, Yi Chang, and Björn W. Schuller. Fast Yet Effective Speech Emotion Recognition with Self-distillation. 5 pages. https://arxiv.org/abs/2210.14636

## Citation

```
@misc{ren2022fast,
      title={Fast Yet Effective Speech Emotion Recognition with Self-distillation}, 
      author={Zhao Ren and Thanh Tam Nguyen and Yi Chang and Björn W. Schuller},
      year={2022},
      eprint={2210.14636},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}
```

## Abstract

In this paper, self-distillation was applied to produce a fast and effective SER model, by simultaneously fine-tuning wav2vec 2.0 and training its shallower versions.

## Config

In the runme.sh, all of the paths can be set and run corresponding python files via 

>sh runme.sh


<!---
## Cite
If you use the code from this repositroy, please cite the following reference in your paper:

[1] Zhao Ren, Thanh Tam Nguyen, Yi Chang, and Björn W. Schuller. "Fast Yet Effective Speech Emotion Recognition with Self-distillation", arXiv:2210.14636, 2022, 5 pages.
-->
